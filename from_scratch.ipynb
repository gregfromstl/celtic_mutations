{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Celtic Mutations -- From Scratch\n",
    "### Hidden Markov Model\n",
    "\n",
    "For my \"from scratch\" implementation I implemented a standard Hidden Markov Model using\n",
    "python dictionaries. The final model was able to achieve just under 90% accuracy on a validation\n",
    "set taken as 10% of provided training data.\n",
    "\n",
    "*Note: I use state, tag, and label synonymously throughout the code and comments*"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is where all environment variables are declared. **To include test data evaluation, simply\n",
    "assign the relative filepath to the `test_file` variable.** If the `test_file` variable is an empty\n",
    "string the test evaluation will be skipped.\n",
    "\n",
    "The `states` dictionary specifies all the possible states/tags. If one were to be added to the dataset,\n",
    "it can be included in the model simply by adding the label to this dictionary."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_file: str = 'data/train.tsv'\n",
    "test_file: str = ''\n",
    "val_split: float = 0.1\n",
    "\n",
    "states: dict = {\n",
    "    'S': 0,\n",
    "    'U': 0,\n",
    "    'T': 0,\n",
    "    'H': 0,\n",
    "    'N': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `load_data` function constructs sentences by searching for the `<S>` markers.\n",
    "It'll append each word to a `sentence` list until one of these markers is found. Once that happens, it starts\n",
    "a new sentence by appending the `sentence` list to the overall `data` list and re-initializes the `sentence`\n",
    "variable to be filled with the next sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def load_data(file: str) -> list:\n",
    "    print(\"Loading data from file {}...\".format(file))\n",
    "    file = open(file, 'r')\n",
    "    data = []\n",
    "    sentence = []\n",
    "    for line in file:\n",
    "        pieces = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        if pieces[0] == '<S>':\n",
    "            data.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            sentence.append(pieces)\n",
    "    print(\"Loaded {} sentences\".format(len(data)))\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here I load the data from the `train_file` using `load_data`, both specified above. The `test_file`\n",
    "will be loaded only if a filepath was entered.\n",
    "\n",
    "I then split the training and validation data based on the `val_split` variable. During development\n",
    "I set this to 0.1 or 0.3, with little difference in accuracy between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file data/train.tsv...\n",
      "Loaded 395922 sentences\n",
      "Splitting data...\n",
      "39593  validation sentences\n",
      "356329  training sentences\n"
     ]
    }
   ],
   "source": [
    "train_data: list = load_data(train_file)\n",
    "if len(test_file) > 0:\n",
    "    test_data: list = load_data(test_file)\n",
    "print(\"Splitting data...\")\n",
    "num_train_samples: int = int(len(train_data)*(1-val_split))\n",
    "val_data: list = train_data[num_train_samples:]\n",
    "print(len(val_data), \" validation sentences\")\n",
    "train_data: list = train_data[:num_train_samples]\n",
    "print(len(train_data), \" training sentences\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is a utility function that will be used in a number of other functions. It takes a dictionary of\n",
    "\"counts\" representing the counts of occurrences of different features and finds the probability of\n",
    "each occurring in the specific context using the sum of all counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def compute_probabilities_from_counts(counts_dict: dict) -> dict:\n",
    "    counts_sum: int = sum(counts_dict.values())\n",
    "    probabilities_dict: dict = {}\n",
    "    for count_id in counts_dict:\n",
    "        count = counts_dict[count_id]\n",
    "        probabilities_dict[count_id] = count / counts_sum\n",
    "    assert round(sum(probabilities_dict.values()), 2) == 1.0, \"All probabilities should sum to 1 but got {}\".format(round(sum(probabilities_dict.values()), 2))\n",
    "    return probabilities_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is another utility function to be used later. It finds the maximum value in a dictionary and\n",
    "returns its ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def get_max_float_id_from_dict(float_dict: dict) -> str:\n",
    "    max_value: float = -0.1\n",
    "    max_id: float = None\n",
    "    for dict_id in float_dict:\n",
    "        if float_dict[dict_id] > max_value:\n",
    "            max_value = float_dict[dict_id]\n",
    "            max_id = dict_id\n",
    "    return max_id\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This function determines the initial state probabilities of the provided data. It looks at only the\n",
    "first word of each sentence and determines the probability of the first word in any sentence of\n",
    "having each tag. For the training dataset the `N` state/tag was heavily favored, as expected.\n",
    "\n",
    "These probabilities are later combined with the emission state probabilities when evaluating the\n",
    "first word of a sentence, since the transition state cannot be evaluated with no previous state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def generate_initial_state_probabilities(data: list) -> dict:\n",
    "    initial_state_counts: dict = states.copy()\n",
    "    for sentence in data:\n",
    "        initial_state: str = sentence[0][1]\n",
    "        if initial_state in initial_state_counts:\n",
    "            initial_state_counts[initial_state] += 1\n",
    "    initial_state_probabilities: dict = compute_probabilities_from_counts(initial_state_counts)\n",
    "    return initial_state_probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This function generates the probability of going from one state to another using the specified dataset. It does so with\n",
    "a two-level dictionary. By iterating over each sentence but excluding the first (initial state) word,\n",
    "the index provided by `enumerate` will be one behind each word's index in the sentence. Therefore this\n",
    "becomes the `prev_idx` and can retrieve the previous state.\n",
    "\n",
    "The first level of the dictionary is the preceding state and the second is the current state. We initially\n",
    "count all occurrences, then use the above-defined `compute_probabilities_from_counts` function to turn these\n",
    "into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def generate_transition_state_probabilities(data: list) -> dict:\n",
    "    # create a dictionary with two levels, the first being the previous state and the second being the current state\n",
    "    transition_state_counts: dict = {state: states.copy() for state in states}\n",
    "    for sentence in data:\n",
    "        # since we enumerate over a list that excludes the first item, the enumeration index is one behind\n",
    "        for prev_idx, word in enumerate(sentence[1:]):\n",
    "            prev_state: str = sentence[prev_idx][1]\n",
    "            current_state: str = word[1]\n",
    "            if prev_state in transition_state_counts and current_state in transition_state_counts[prev_state]:\n",
    "                transition_state_counts[prev_state][current_state] += 1\n",
    "    transition_state_probabilities: dict = {state: {} for state in states}\n",
    "    for prev_state in transition_state_counts:\n",
    "        transition_state_probabilities[prev_state] = compute_probabilities_from_counts(transition_state_counts[prev_state])\n",
    "    return transition_state_probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This function generates the emission probability for each word given each state using another two-level dictionary.\n",
    "It iterates over each word and counts the occurrence of word X in state Y's dictionary. It then\n",
    "uses the `compute_probabilities_from_counts` function to get the **generative probabilities**. That is,\n",
    "given a state/tag, how likely will that state/tag generate each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def generate_emission_probabilities(data: list) -> dict:\n",
    "    emission_counts_by_state: dict = {state: {} for state in states}\n",
    "    for sentence in data:\n",
    "        for word_state_pair in sentence:\n",
    "            word, state = word_state_pair\n",
    "            if state in emission_counts_by_state:\n",
    "                # initialize word in state dict if the first occurrence of word X in state Y\n",
    "                if word not in emission_counts_by_state[state]:\n",
    "                    emission_counts_by_state[state][word] = 0\n",
    "                emission_counts_by_state[state][word] += 1\n",
    "    emission_probabilities_by_state: dict = {state: {} for state in states}\n",
    "    for state in emission_counts_by_state:\n",
    "        emission_probabilities_by_state[state] = compute_probabilities_from_counts(emission_counts_by_state[state])\n",
    "    return emission_probabilities_by_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The HMM class has four methods:\n",
    "- The `fit` method is run with `init` and simply runs all the probability-generating functions\n",
    "defined above with the provided data.\n",
    "- The `format_evaluation_data` method is used to convert the datasets from the format used\n",
    "to fit a model into the format used by the `evaluate` method.\n",
    "- The `predict` method is the most important part of the class. It takes a single sentence\n",
    "as a string and uses the probability dictionaries to generate a predicted state sequence. It begins\n",
    "by initializing `state_probabilities` as the `initial_state_probabilities`. This is because\n",
    "there is no transition state probability with the first word. For all subsequent words `state_probabilities`\n",
    "are initialized as the transition state probabilities based on the most likely previous state.\n",
    "Each of the initial state / transition state probabilities are then multiplied by the emission probabilities.\n",
    "*If a word hasn't appeared with any given state, its overall probability is set to 0. I know this isn't\n",
    "smoothing and in an ideal situation / if I get time later it will be.* I tried a form of smoothing where it took\n",
    "the minimum emission probability but this dramatically increased runtime.\n",
    "- The `evaluate` method simply takes a list of sentences as strings and a list of lists of labels\n",
    "as strings and runs predict on each sentence, comparing the outputs to the given labels and computing\n",
    "accuracy as it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, data):\n",
    "        self.initial_state_probabilities, self.transition_probabilities, self.emission_probabilities = self.fit(data)\n",
    "\n",
    "    @staticmethod\n",
    "    def fit(data: list) -> tuple:\n",
    "        print(\"Fitting model to provided dataset...\")\n",
    "        initial_state_probabilities = generate_initial_state_probabilities(data)\n",
    "        transition_probabilities = generate_transition_state_probabilities(data)\n",
    "        emission_probabilities = generate_emission_probabilities(data)\n",
    "        print(\"Model ready.\")\n",
    "        return initial_state_probabilities, transition_probabilities, emission_probabilities\n",
    "\n",
    "    def format_data(self, evaluation_data: list) -> tuple:\n",
    "        sentences: list = []\n",
    "        labels: list = []\n",
    "        for sentence in evaluation_data:\n",
    "            sentences.append((\" \".join(word_state_pair[0] for word_state_pair in sentence)))\n",
    "            labels.append([word_state_pair[1] for word_state_pair in sentence])\n",
    "        return sentences, labels\n",
    "\n",
    "    def predict(self, sentence: str) -> dict:\n",
    "        words: list = sentence.split(\" \")\n",
    "        state_sequence: list = []\n",
    "        # begin each sentence using initial state probabilities, then switches to transition probabilities\n",
    "        state_probabilities: dict = self.initial_state_probabilities.copy()\n",
    "        for idx, word in enumerate(words):\n",
    "            for state in state_probabilities:\n",
    "                if word in self.emission_probabilities[state]:\n",
    "                    state_probabilities[state] = state_probabilities[state]*self.emission_probabilities[state][word]\n",
    "                else:\n",
    "                    state_probabilities[state] = 0\n",
    "            state_sequence.append(get_max_float_id_from_dict(state_probabilities))\n",
    "            # for next word, initialize probabilities as transition probabilities from the previous state\n",
    "            state_probabilities = self.transition_probabilities[state_sequence[idx]].copy()\n",
    "        return state_sequence\n",
    "\n",
    "    def evaluate(self, evaluation_data: list) -> float:\n",
    "        sentences, labels = self.format_data(evaluation_data)\n",
    "        total, correct = 0, 0\n",
    "        print(\"Evaluating {} sentences...\".format(len(sentences)))\n",
    "        for sentence_idx, sentence in enumerate(sentences):\n",
    "            predicted_sequence: list = self.predict(sentence)\n",
    "            total += len(predicted_sequence)\n",
    "            correct += sum([int(predicted == labels[sentence_idx][tag_idx]) for tag_idx, predicted in enumerate(predicted_sequence)])\n",
    "        print(\"Done! {} correct tags out of {} words\".format(correct, total))\n",
    "        return correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create the model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model to provided dataset...\n",
      "Model ready.\n"
     ]
    }
   ],
   "source": [
    "model = HMM(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A couple use case sentences where the model picks up on the mutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: d'fhan sé ina tost ar feadh scaithimh eile sular labhair sé .\n",
      "Predicted Sequence: ['N', 'N', 'N', 'S', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']\n",
      "Ground Truth Sequence: ['N', 'N', 'N', 'S', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']\n"
     ]
    }
   ],
   "source": [
    "val_sentence = \" \".join([word[0] for word in val_data[100]])\n",
    "print(\"Sentence:\", val_sentence)\n",
    "print(\"Predicted Sequence:\", model.predict(val_sentence))\n",
    "print(\"Ground Truth Sequence:\", [word[1] for word in val_data[100]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: mill an séasúr seo caite muid .\n",
      "Predicted Sequence: ['S', 'N', 'N', 'N', 'N', 'N', 'N']\n",
      "Ground Truth Sequence: ['S', 'N', 'N', 'N', 'N', 'N', 'N']\n"
     ]
    }
   ],
   "source": [
    "val_sentence = \" \".join([word[0] for word in val_data[1000]])\n",
    "print(\"Sentence:\", val_sentence)\n",
    "print(\"Predicted Sequence:\", model.predict(val_sentence))\n",
    "print(\"Ground Truth Sequence:\", [word[1] for word in val_data[1000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Evaluate validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 39593 sentences...\n",
      "Done! 865363 correct tags out of 963502 words\n",
      "Validation Accuracy:  89.81%\n"
     ]
    }
   ],
   "source": [
    "val_acc = model.evaluate(val_data)\n",
    "print(\"Validation Accuracy: \", str(round(val_acc*100, 2)) + \"%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate testing set if specified."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "if \"test_data\" in globals():\n",
    "    test_acc = model.evaluate(test_data)\n",
    "    print(\"Testing Accuracy: \", str(round(test_acc*100, 2)) + \"%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}